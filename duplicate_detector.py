#pip install sentence-transformers pandas tqdm
from textwrap import dedent
import os
import json
import argparse
from pathlib import Path
from typing import List, Dict
import pandas as pd
from sentence_transformers import SentenceTransformer, util

# --- CONFIGURATION ---
DEFAULT_DIR = "/Users/leigh_greathouse/Documents/My_Code/Python_code/qa_jsonl"
# Using a model pre-trained on medical/scientific text is often better, 
# but 'all-MiniLM-L6-v2' is the standard for speed/accuracy balance.
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2' 

def parse_custom_jsonl(file_path: Path) -> List[Dict]:
    """
    Parses the specific 'pretty JSON' format generated by mupdf_trainer_v2.py
    which uses '---' to separate records.
    """
    qa_records = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # The generator script uses "\n---\n" as a separator
        if '---' in content:
            chunks = content.split('---')
        else:
            # Fallback for standard newline-delimited JSONL
            chunks = content.splitlines()

        for chunk in chunks:
            chunk = chunk.strip() #clean whitespace
            if not chunk:
                continue
            try:
                # Attempt to parse the JSON block
                data = json.loads(chunk) # parses the JSON into a python dict
                if isinstance(data, dict) and 'question' in data: # requires object and class information and returns True
                    # Inject filename for traceability
                    data['_source_file'] = file_path.name #injects a new key into the dictionary so we know where this record came from later.
                    qa_records.append(data)
            except json.JSONDecodeError: #if it fails to parse, skip it
                continue
                
    except Exception as e: #catch all for file read errors
        print(f"Error reading {file_path.name}: {e}")
        
    return qa_records #returns a list of dictionaries of all the question/answer pairs found in the files

def find_duplicates(input_dir: str, threshold: float = 0.85, output_csv: str = "duplicate_report.csv"):
    """
    Main logic: Load data -> Embed -> Similarity Search -> Report
    """
    root_path = Path(input_dir)
    if not root_path.exists():
        print(f"Directory not found: {input_dir}")
        return

    # 1. Load Data
    print(f"Scanning {root_path} for *_qa.jsonl files...")
    all_records = []
    files = list(root_path.glob("*_qa.jsonl"))
    # globe is like ls *.jsonl but in python
    # list() converts the generator to a list

    for p in files:
        records = parse_custom_jsonl(p)
        all_records.extend(records) #extend adds the contents of records to all_records as opposed to adding the list as a single item with append

    if not all_records:
        print("No records found.")
        return

    print(f"Loaded {len(all_records)} Q/A pairs. Loading embedding model {MODEL_NAME}...")

    # 2. Load Model
    # 'question' is the primary key for semantic duplication
    questions = [r['question'] for r in all_records]
    # read the 'question' field from each record into a list called questions, where r is the loop variable representing each record in all_records
    model = SentenceTransformer(MODEL_NAME)

    # 3. Mine Paraphrases
    # paraphrase_mining is optimized for finding high-similarity pairs in large lists
    # It will compute embeddings internally
    print(f"Computing embeddings and identifying pairs with similarity > {threshold}...")
    duplicates = util.paraphrase_mining(model, questions, show_progress_bar=True, batch_size=64, top_k=10)
    # duplicates is a list of tuples (score, idx1, idx2) where idx1 and idx2 are indices in the original questions list

    # 4. Filter and Structure Output
    report_data = []
    
    for score, idx1, idx2 in duplicates:
        if score < threshold:
            continue
            
        rec1 = all_records[idx1]
        rec2 = all_records[idx2]
        
        # If questions are identical strings, score is 1.0. 
        # If semantic duplicates, score is ~0.85-0.99.
        status = "Exact Duplicate" if score > 0.99 else "Semantic Similar"
        
        report_data.append({
            "similarity_score": round(score, 4),
            "status": status,
            "q1_id": rec1.get('qa_id', 'N/A'),
            "q1_file": rec1['_source_file'],
            "question_1": rec1['question'],
            "q2_id": rec2.get('qa_id', 'N/A'),
            "q2_file": rec2['_source_file'],
            "question_2": rec2['question']
        })

    # 5. Save Report
    if report_data:
        df = pd.DataFrame(report_data) #converts a list of dictionaries into a table (rows and columns).
        # Sort by score descending to show worst offenders first
        df = df.sort_values(by="similarity_score", ascending=False)
        
        out_path = root_path / output_csv #creates a full path for the output file by combining the directory path and the output filename
        df.to_csv(out_path, index=False)
        
        print(f"\nAnalysis Complete.")
        print(f"Found {len(report_data)} similar pairs.")
        print(f"Report saved to: {out_path}")
        print("Review this CSV to decide which pairs to remove.")
    else:
        print("\nNo duplicates found above the threshold.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Scan QA JSONL files for semantic duplicates.")
    parser.add_argument("--dir", default=DEFAULT_DIR, help="Directory containing .jsonl files")
    parser.add_argument("--threshold", type=float, default=0.85, help="Similarity threshold (0.0-1.0)")
    parser.add_argument("--out", default="duplicates_report.csv", help="Output CSV filename")
    
    args = parser.parse_args()
    
    find_duplicates(args.dir, args.threshold, args.out)
